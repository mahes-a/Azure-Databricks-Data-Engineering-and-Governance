# Delta Live Tables Medallion Data Pipeline 

This tutorial shows you how to configure a Delta Live Tables data pipeline from code in a Databricks notebook and to trigger an update. The instructions provided are general enough to cover most notebooks with properly-defined Delta Live Tables syntax.

You can configure Delta Live Tables pipelines and trigger updates using the Azure Databricks workspace UI or automated tooling options such as the API and CLI. Databricks recommends familiarizing yourself with the UI first, which can be used to generate JSON configuration files for programmatic execution.

## Important

To start a pipeline, you must have cluster creation permission or access to a cluster policy defining a Delta Live Tables cluster. The Delta Live Tables runtime creates a cluster before it runs your pipeline and fails if you don’t have the correct permission.

To run this example, select Hive metastore as the storage option when you create the pipeline. Because this example reads data from DBFS, you cannot run this example with a pipeline configured to use Unity Catalog as the storage option.

## Create a pipeline

Delta Live Tables creates pipelines by resolving dependencies defined in notebooks or files (called source code or libraries) using Delta Live Tables syntax. 

- Click Jobs Icon Workflows in the sidebar, click the Delta Live Tables tab, and click Create Pipeline.

 <img width="1042" alt="image" src="https://github.com/mahes-a/Azure-Databricks-Data-Engineering-and-Governance/assets/120069348/9f3b32b7-d2a0-4d43-be0a-336c51c0ed20">

-  Give the pipeline a name and click File Picker Icon to select the 08.DeltaLiveMedallion.py notebook which should be imported from the repo

  <img width="925" alt="image" src="https://github.com/mahes-a/Azure-Databricks-Data-Engineering-and-Governance/assets/120069348/34ea0a37-ba95-47aa-92d6-f5c5db7702a3">

  
-  Select Triggered for Pipeline Mode.
-  (Optional) Enter a Storage location for output data from the pipeline. The system uses a default location if you leave Storage location empty.
-  (Optional) Specify a Target schema to publish your dataset to the Hive metastore or a Catalog and a Target schema to publish your dataset to Unity Catalog. See Publish datasets.
-  (Optional) Click Add notification to configure one or more email addresses to receive notifications for pipeline events. See Add email notifications for pipeline events.
-  Click Create.

The system displays the Pipeline Details page after you click Create. You can also access your pipeline by clicking the pipeline name in the Delta Live Tables tab.

 <img width="1044" alt="image" src="https://github.com/mahes-a/Azure-Databricks-Data-Engineering-and-Governance/assets/120069348/8934bedb-d5ef-40bc-ae86-13157b72bd82">


## Start a pipeline update

To start an update for a pipeline, click the Delta Live Tables Start Icon button in the top panel. The system returns a message confirming that your pipeline is starting.

After successfully starting the update, the Delta Live Tables system:

1. Starts a cluster using a cluster configuration created by the Delta Live Tables system. You can also specify a custom cluster configuration.
2. Creates any tables that don’t exist and ensures that the schema is correct for any existing tables.
3. Updates tables with the latest data available.
4. Shuts down the cluster when the update is complete.

   <img width="691" alt="image" src="https://github.com/mahes-a/Azure-Databricks-Data-Engineering-and-Governance/assets/120069348/06087d62-f90c-4d72-b726-025cbfab3ceb">

    <img width="1042" alt="image" src="https://github.com/mahes-a/Azure-Databricks-Data-Engineering-and-Governance/assets/120069348/39a8e7dd-5702-4286-ab6a-5c9e62bd6210">



